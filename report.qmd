---
title: "Teaching Math Effectively: Insights from LSAY and PISA on Primary Teacher Proficiency"
author: Tashya Sathyajit (29672732)
format: 
  pdf:
    number-sections: true
    toc: true
    include-in-header:
      text: |
        \usepackage{lipsum}
        \usepackage{setspace}
        \onehalfspacing
        \linespread{2}
    df-print: kable
    highlight-style: zenburn
fontsize: 12pt
geometry: margin=1in
execute: 
  echo: false
  warning: false
  message: false
  wordcount: true
---


```{r}
# necessary packages
library(dplyr)
library(kableExtra)
library(knitr)
```

# Abstract

This report investigates the mathematics performance of individuals who became primary school teachers in Australia, using the Longitudinal Surveys of Australian Youth (LSAY) and the Program For International Student Assessment (PISA) datasets. This analysis was done for the contribution to Grattan Institute's upcoming report on teacher proficiency in mathematics. Results show that teachers consistently have a higher average math proficiency than non-teachers over time, with better performance across PISA testing years. However, they tend to score lower than professionals, particularly in STEM fields. Limitations include disproportionate teacher counts in some PISA years, potential attrition bias, confounding variables such as socioeconomic status, and scope of PISA. To view the full repository for this analysis, visit [here](https://github.com/tsat0002/pisa-teacher-proficiency-analysis).

# Background and motivation

Over the past two decades, Australian students' performance in the mathematics components of international assessments like PISA have shown significant declines (Morsy et al., 2018) as seen in @fig-pisadecline. These declines persist even after adjusting for changes in family academic resources, affecting students across all states, social groups, and school types (albeit with variations). This trend has raised concerns among educators, researchers and policymakers as mathematics proficiency is not only foundational for broader academic success but crucial for Australia's future economic competitiveness.

Research indicates a positive correlation between teacher mathematics knowledge (as assessed through certification exams or subject-matter tests) and student achievement (Hill et al., 2005) . This underscores the importance of qualified primary school teachers in improving children's math outcomes, supporting the notion that teachers with stronger mathematical knowledge are likely to teach it more effectively (Norton, 2019).

Research shows that a teacher's proficiency in mathematics is key to classroom effectiveness and student achievement (Morsy et al., 2018). However this link between subject knowledge and student performance is more complex than just content mastery. It involves the ability to translate that knowledge into effective pedagogical practices, adapt to curriculum needs, engage in productive professional conversations and address student misconceptions. When students are taught by teachers who have little mathematical content *and* pedagogical content knowledge, learning suffers (Baumert et al., 2010; Hill et al., 2005).

Despite these nuances, the positive correlation between teachers' competence in mathematics and student achievement is clear (Hill et al., 2005). While this correlation does not strictly imply causation, understanding the mathematics proficiency of primary school teachers remains a critical step in improving student outcomes.

This analysis aims to enhance understanding of the mathematics proficiency of individuals who became primary school teachers in Australia, using longitudinal data from PISA and LSAY. By examining these trends over time and comparing the math performance of these teachers with the broader Australian population and other professionals, the study provides insights into teachers' performance in mathematics from ages fifteen to twenty five. These findings can guide professional development initiatives that not only target teachers' math skills but support other competencies crucial for effective math instruction.


![Decline in PISA Scores Across the Years](images/pisadecline.png){#fig-pisadecline}


# Objectives and significance

As aforementioned, the primary objective of this analysis was to utilise longitudinal survey data to understand the mathematics proficiency of students who eventually became primary school teachers in Australia. Using the LSAY and PISA datasets, the analysis seeks to address three main questions:

1.  **Teacher performance over time:** How well did individuals who became primary school teachers perform in mathematics across the years?
2.  **Comparison to other Professionals:** How are primary school teachers performing in comparison to other 'professionals'?
3.  **Comparison to the General Population:** How do teachers' math scores compare to the rest of the sample (non-primary school teachers) each year?


# Methodology

Data was sourced from multiple LSAY cohorts: 2003, 2006, 2009 and 2015. Each dataset contains demographic data from student responses and their linked performance in PISA. These years were selected because from 2003 onwards, LSAY participants were recruited from schools that also took part in PISA, allowing for the linkage of PISA scores to LSAY data.

## How did we identify primary school teachers?

To identify primary school teachers for each cohort, we examined variables related to occupation across each LSAY cohort. The [data dictionary](https://www.lsay.edu.au/data/lsay-data-dictionary/data-dictionary) was used to locate the relevant variables, as found in @tbl-kow. These variables correspond to responses about the type of work the respondents did.

Individuals were flagged as primary school teachers if they reported an ANZSCO code of `2412`, which represents primary school teacher. This flag distinguished those who eventually became primary school teachers from those who did not. Respondents with other occupation codes were classified as non-teachers.

```{r}
#| label: tbl-kow
#| tbl-cap: "Variables for 'What kind of work do you do in this job?' Questions"
kow <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LDD025, LED025, LFD025, LGD025, LHD029, LID029, LJD029, LKD029",
    "ANZSCO, LBD011, LCD024, LDD024, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "ANZSCO73, LBD014, LCD029, LDD029, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "LSAYID, LBEM004, LCD022, LDD022, LED022, LFD025, LGD022, LHD022"
  )
)

kable(kow, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(kow), hline_after = TRUE)

```

### What did these teachers study? 

To understand the educational backgrounds of individuals who later became primary school teachers, fields of study were analysed using variables detailed in @tbl-aos. The analysis focused on identifying whether respondents pursued primary school education, coded as `7301` in accordance with the [Australian Standard Classification of Education (ASCED)](https://www.abs.gov.au/statistics/classifications/australian-standard-classification-education-asced/latest-release).

Additionally, the analysis explored whether respondents who became teachers had previously studied fields outside of education before transitioning to teaching. This was done with a similar approach of examining ASCED scores reported across waves of each cohort for those who were identified as teachers. @tbl-aosunique provides a summary of the fields of study identified and highlights cases where individuals shifted from other fields to primary education.

::: {.callout-tip}
## What Are PISA Waves?
PISA waves refer to the rounds of data collection conducted by the OECD for each cohort every three years from age 15 to 25. For example, the first wave of the 2003 cohort would be when the respondents were 15 years old, and the final wave would be when they were 25 years old. 
:::


```{r}
#| label: tbl-aos
#| tbl-cap: "Variables for Main Area of Study Questions"
aos <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LBCZ087, LCCZ087, LDCZ087, LEC087, LFC087, LGC087, LHC087, LIC087, LJC088, LKC088",
    "LBC087, LCC087, LDC087, LEC087, LFC087, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LBC089, LCC088, LDC088, LEC088, LFC088, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LCC088, LDC088, LEC088, LFC088, LGC088, LHC088"
  )
)

kable(aos, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(aos), hline_after = TRUE)
```


```{r}
#| label: tbl-aosunique
aos_unique <- data.frame(
  Code = c(
    70101, 70103, 70109, 70199, 70115, 90503, 91703, 92101, 100101, 80101,
    100705, 100103, 80399, 70105, 91701, 60901, 61399, 60101, 81101, 91523, 90513,
    90701, 79999, 60501, 61799
  ),
  Field_of_Education = c(
    "Teacher Education: Early Childhood",
    "Teacher Education: Primary",
    "Teacher Education: Vocational Education and Training",
    "Teacher Education, n.e.c.",
    "English as a Second Language Teaching",
    "Children's Services",
    "Religious Studies",
    "Sport and Recreation Activities",
    "Music",
    "Accounting",
    "Written Communication",
    "Drama and Theatre Studies",
    "Business and Management, n.e.c.",
    "Teacher Education: Secondary",
    "Philosophy",
    "Optometry",
    "Public Health, n.e.c.",
    "General Medicine",
    "Banking and Finance",
    "Literature",
    "Counselling",
    "Psychology",
    "Education, n.e.c.",
    "Pharmacy",
    "Rehabilitation Therapies, n.e.c."
  )
)

kable(aos_unique, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(aos_unique), hline_after = TRUE)
```


## How did we explore teacher performance relative to other professionals?

This part of the analysis was driven more by curiosity than formal research aims, exploring how primary school teachers' mathematics proficiency compares to that of other professionals. Due to this, the analysis focused only on the 2003 PISA cohort which had the largest sample of teachers (view @fig-teachercount) and of that cohort, the final wave. By examining the last wave of this cohort, the analysis captures participants' final professional roles, though this approach results in a lower teacher count than the total count for the entire 2003 cohort as seen in @tbl-professionals.

To define professional categories, the analysis used the ANZSCO codes sourced from the Australian Bureau of Statistics [website](https://www.abs.gov.au/statistics/classifications/anzsco-australian-and-new-zealand-standard-classification-occupations/latest-release). The respondent's occupation code response to variable `LKD029` corresponding to LSAY question relating to kind of work for that wave was used to identify whether they belonged to one of the professional groups of interest. A new variable was established which created a flag for individuals based on their occupation code (e.g. Teachers coded as '1', while other professional groups were coded '2' to '6' in accordance to their ANZSCO code). 


```{r}
#| label: tbl-professionals

professionals_table <- data.frame(
  Professionals = c("Primary School Teacher", 
                    "Medical Practitioners", 
                    "Midwifery and Nursing Professionals", 
                    "Legal Professionals", 
                    "Accounts Auditors and Company Secretaries", 
                    "Engineering Professionals"),
  ANZSCO_Code = c(2412, 253, 254, 271, 221, 233),  
  Count_n = c(82, 48, 94, 50, 92, 88)  
)

professionals_table <- professionals_table %>%
  rename(
    `ANZSCO Code` = ANZSCO_Code,
    Count = Count_n
  )


professionals_table %>%
  kable("latex", booktabs = TRUE, caption = "Professionals, Corresponding ANZSCO Codes, and Counts") %>%
  kable_styling(latex_options = c("striped"))

```


## How did we complete the Mathematical Proficiency Analysis?

The mathematics performance of primary school teachers was assessed using PISA scores. The analysis aimed to compare teachers' scores against the national proficiency standard which has been set at a [proficiency level 3](https://www.oecd-ilibrary.org/sites/5f07c754-en/1/2/7/index.html?itemId=/content/publication/5f07c754-en&_csp_=6aa84fb981b29e81b35b3f982f80670e&itemIGO=oecd&itemContentType=book) or [482 PISA points](https://www.parliament.nsw.gov.au/researchpapers/Documents/PISA%20paper.pdf). The analysis incorporated survey weights to ensure representativeness of the data and address differences in sample sizes across cohorts. It also had to account for the use of plausible values undertaken by PISA.

::: {.callout-note}
## **What is a Level 3 Proficiency in Mathematics?**
"At Level 3, students can execute clearly described procedures, including those that require sequential decisions. Their interpretations are sufficiently sound to be a base for building a simple model or for selecting and applying simple problem-solving strategies. Students at this level can interpret and use representations based on different information sources and reason directly from them. They typically show some ability to handle percentages, fractions and decimal numbers, and to work with proportional relationships. Their solutions reflect that they have engaged in basic interpretation and reasoning."
:::

## Working with Sampling Weights and Plausible Values Accurately

International surveys such as PISA report student performance using plausible values (PVs). PVs are a statistical technique that generates multiple imputed estimates of a student's proficiency rather than a single estimate, helping to represent measurement uncertainty. This approach assists in conveying the variability in student performance more accurately, considering that not all students answer the same questions (PISA is a large test) or have the same circumstances.

To derive PVs, PISA uses [Item Response Theory (IRT)](https://www.oecd-ilibrary.org/docserver/c224dbeb-en.pdf?expires=1730041650&id=id&accname=guest&checksum=6717B0D94EE373655D9733F88B98A43F), which predicts the likelihood of a student answering a question correctly by assessing the difficulty of each question and how well it distinguishes between students of different ability levels. For example, if the question is deemed a relatively conceptually 'easy' question, and the estimated ability of the student is high, the model predicts a high probability of a correct response. PISA can determine if a question is 'easy' or 'hard' based on the statistical analysis of response patterns from a broad group of students, using IRT to estimate how challenging each question is relative to the separate proficiency levels, as can be seen [here](https://www.oecd-ilibrary.org/docserver/02f44b44-en.pdf?expires=1730101739&id=id&accname=guest&checksum=903DBFF4325A31EBBCC87BE873992B35) (OECD, 2023).

Looking at @fig-pvs retrieved from [Wu (2022)](https://www.edmeasurementsurveys.com/Ch14.html), the process starts with a prior distribution. This is an initial estimate of the student's proficiency. This prior estimation serves as a rough starting point that as the students begin to answer questions, gets finetuned as PISA updates this estimate using student responses and resulting in a posterior distribution. This updating distribution combines that initial prior estimate with the observed test results.

After this, multiple imputed values are taken from this posterior distribution. These plausible values are thus a representation of the range of abilities a student may reasonably have (OECD, 2009) and are normally five to ten plausible values per student for each domain.


![Prior and Posterior Distributions](images/pvs.png){#fig-pvs}


The [`intsvy`](https://github.com/eldafani/intsvy) and [`rrepest`](https://github.com/cran/Rrepest) R packages were used to manage the complex survey design of PISA data, ensuring accurate estimation of statistics such as mean scores, confidence intervals, and standard errors while incorporating plausible values and replicate weights. These packages enabled the calculation of weighted averages, accounting for the distribution of teachers and non-teachers in each cohort to provide representative estimates of proficiency scores.

Given PISA's stratified, multi-stage sampling design which is adjusted for non-response, school selection, and student participation, the `Rrepest` package accurately applies sampling weights when calculating summary statistics. It iteratively uses replicate weights to generate multiple estimates, which are then combined to produce standard errors that reflect the survey’s variability.

For this analysis, the `Rrepest` package was primarily used to derive summary statistics, with results cross-checked using the `intsvy` package. Functions in `Rrepest` cycled through each plausible value, averaging results while applying survey weights to ensure valid estimates. The OECD [recommends](https://www.oecd.org/en/about/programmes/pisa/how-to-prepare-and-analyse-the-pisa-database.html) either packages for handling PISA data.

# Results

As earlier mentioned in the report, the project aimed to explore three key questions regarding the mathematical performance of individuals who eventually became primary school teachers.

### How well did individuals who became primary school teachers perform in mathematics across the years?

The analysis reveals that individuals who eventually became primary school teachers consistently performed better in mathematics compared to their non-teaching peers over the years. To quantify this, the weighted average proficiency of teachers and non-teachers was calculated, considering the total number of each group to try achieve more accurate average scores.

The results, as shown in @fig-weightedprof, indicate that just over 80% of teachers met or exceeded the national proficiency standard of 482 PISA points, compared to just over 60% of non-teachers. This substantial difference suggests that individuals who eventually become primary school teachers possess stronger mathematical skills than their non-teaching peers from the same cohort. These results highlight the higher mathematical readiness of future teachers, which may have implications for their effectiveness in the classroom.

However, an important limitation to consider is that the smaller sample of teachers may reduce the observed variability, potentially skewing results if high-performing individuals are overrepresented. @fig-probdist shows that in the 2003 sample, there is less variability within the teacher group compared to the non-teacher group. This potential selection bias could lead to an inflated perception of average teacher math proficiency, as the sample might not fully capture the broader range of abilities likely found in the general teacher population.


```{r}
#| label: tbl-profscores
pisa_results <- data.frame(
  Year = c(2003, 2003, 2006, 2006, 2009, 2009, 2015, 2015),
  Teacher = c("False", "True", "False", "True", "False", "True", "False", "True"),
  Mean = c(530.3, 554.0, 518.6, 550.7, 513.7, 552.7, 494.0, 519.4),
  SE_Mean = c(2.4, 10.5, 3.0, 10.4, 2.7, 7.2, 1.8, 20.2),
  Std = c(92.9, 64.8, 87.9, 67.2, 95.1, 63.9, 92.5, 61.0),
  SE_Std = c(2.4, 5.3, 1.2, 8.2, 2.5, 8.4, 0.9, 19.0)
)
pisa_results %>%
  kable("latex", booktabs = TRUE, caption = "PISA Teacher vs Non-Teacher Math Proficiency") %>%
  kable_styling(latex_options = c("striped"))

```



![Weighted Average of Performance Across Years: Teachers vs Non-Teachers](images/weightedprof.png){#fig-weightedprof}

![Probability Distribution of 2003 Mathematics PISA Scores](images/probdist.png){#fig-probdist}


### How do teachers' math scores compare to the rest of the sample (non primary school teachers)?

@fig-acrossyears shows that teachers tend to perform better than non-teachers across each year of testing.

This is demonstrated by a consistently higher median score for teachers (red) compared to non-teachers (orange) over the years. The boxplots reveal differences in score distribution, with non-teachers showing longer whiskers, indicating a broader range of math scores and greater variability. This wider spread suggests that non-teachers have a more diverse range of mathematical abilities.

In contrast, the boxplots for teachers display slightly shorter whiskers, indicating more consistent scores among this group. However, as seen in @tbl-profscores, the standard error (SE) for teachers is higher, likely due to the smaller sample size. This increases the uncertainty in estimating their average score but does not imply greater variability within the teacher group.

![Proficiency of Teachers vs Non-Teachers Across Years](images/profacrossyears.png){#fig-acrossyears}

### How are teachers performing in comparison to other 'professionals'?

The project had a smaller focus on understanding how teachers' mathematical proficiency compared to other professionals, driven largely by curiosity rather than formal research aims. The selection of professional categories was based on existing reports, and the analysis was conducted on the 2003 PISA cohort, the year with the largest sample of teachers. However this still had its limitations due to the smaller sample size of the relative professional groups within a total sample of 10,370 as seen in @tbl-professionals.

@fig-professionals represents the average PISA scores for teachers and other professionals, along with 90 percent confidence intervals. The analysis suggested that teachers performed lower than most other professional groups in terms of average mathematical proficiency at age 15. Professions such as medical practitioners, legal professionals, engineers and auditors scored higher on average, indicating better mathematical capability among these groups. However, teachers scored higher than the 'Other' category, which represents the broader population not within aforementioned professions. This suggests that while teachers were not among the highest performing professional groups, they seemed to demonstrate better mathematical proficiency than the general cohort.

Despite these findings, it is important to consider the larger confidence intervals observed for some professional groups, such as legal professionals, engineers and auditors. These wider intervals primarily reflect greater uncertainty in the estimates due to smaller sample sizes and potential sample bias within these groups. This increased uncertainty makes precise comparisons more challenging and introduces difficulty to ascertain with confidence the true average performance of these professional groups.

![Teachers in Comparison to Other Professionals](images/professionals.png){#fig-professionals}


# Discussion, Limitations and Future Research


The findings of this analysis provide insights into the mathematical proficiency of individuals who eventually become primary school teachers, indicating a generally higher proficiency than their non-teaching peers across multiple years. Teachers tend to exceed  national proficiency standards more frequently than non-teachers, suggesting a relatively stronger foundation in mathematical skills. This trend is evident over time, as teachers consistently achieve higher median scores with less variability compared to non-teachers, as seen in @fig-acrossyears.

The more consistent performance among teachers is suggested by the slightly shorter whiskers in the boxplots, indicating a relatively uniform level of mathematical competence. In contrast, the longer whiskers for non-teachers reflect greater variability, indicating a broader range of skills. However, it is important to note that the whiskers for both groups are relatively long, with non-teachers demonstrating only slightly more spread. For the teacher boxplots, this may be due to the higher standard error due to their significantly smaller sample size, increasing uncertainty in the estimated average scores.

When comparing teachers to other professional groups, as seen in @fig-professionals, the analysis reveals that while future teachers performed better than the general population, they performed lower than medical practitioners, legal professionals, engineers, and auditors and performed quite similarly to midwifery and nursing professionals. This aligned with a preconceived idea that STEM professions would attract students with more confidence or proficiency in mathematics. However, the large confidence intervals observed for some professional groups highlight greater uncertainty of where the true average lies due to smaller sample sizes and potential variability within these groups.

Despite these findings, there are some considerations that are important to pay mind to upon the results of this analysis. Despite the relatively strong performance of teachers in mathematics, as aforementioned, effective teaching requires more than just content knowledge. Research frameworks emphasise that teachers must understand how to present content effectively, integrate it with the curriculum, and adapt it to diverse learning styles (Norton, 2018). Additionally, although we can see that individuals who became teachers in later life had a higher proficiency in mathematics when compared to the general population (excluding the professions explored in more detail), there are limitations to the analysis.


![Count of Teachers per Year](images/teachercount.png){#fig-teachercount}


**These limitations include:**


**Small sample size for teachers and professional groups:**

The sample size for teachers (@fig-teachercount) and other professionals (@tbl-professionals) was relatively small, which limits the robustness of the findings. The smaller sample size is also associated with larger standard errors, increasing uncertainty in the estimated averages of those groups. The lack of statistical power due to small sample sizes may have affected the precision of the estimates, potentially leading to biased results or challenges in generalising the results.

**Attrition Rate in Longitudinal Data:**

The analysis is based on longitudinal data, which is inherently prone to attrition over time. As participants drop out of the study, the sample size decreases, which may introduce potential bias and affect the reliability of the results. Further, the later PISA cohorts have not completed all the waves which contributes to the smaller sample size for teachers in the later years. This attrition may cause a biased sample and may have implications for the generalisability of the findings.

**Scope of PISA:**

As mentioned prior, while PISA tests part of the expected learning outcomes for mathematics, it does not test how good an individual will be at conveying this knowledge to others or the entire range of mathematical knowledge required in teacher training or professional practice. This could suggest that although individuals who performed well in PISA who later became teachers may be good at mathematics, they may not necessarily be good at teaching it.

Additionally, PISA only tracks individuals from the age of 15 to 25, and this does not account for the many other ways individuals go into the pathway of being teachers. For example, mathematical proficiency can be enhanced through alternative pathways or lived work experience before the individual moved into teaching. This could also suggest that the analysis may not capture the full range of individuals who become teachers and their mathematical proficiency.

**Statistical significance:**

For the project, to determine whether differences in mathematical proficiency between teachers and non-teachers statistically significant, a series of Welch's t-tests were conducted for each year. A Welch's t-test was deemed sufficient for this analysis, as it accommodates differences variances and sample sizes between groups. The t-tests generally showed statistically significant differences in average scores, with teachers consistently outperforming non-teachers in the years analysed. However, this should be treated as preliminary statistical analysis due to limitations prior mentioned and methodological considerations which ultimately resulted in the reasoning to not include the results of these t-tests within the report. 

One reasoning of which is the t-test assumes a normal distribution of scores within each group, but this assumption may not hold, especially given the small teacher samples. This could affect the validity of the test results. As seen in @fig-probdist, which represents *only* the 2003 sample, the distribution of teacher scores (orange) shows slight skewness and a narrower spread, indicating potential deviations from normality. 

Next steps could build upon this with the potential use of non-parametric tests or bootstrapping techniques.

To conclude, this analysis reveals that individuals who later became primary school teachers generally exhibit higher mathematical proficiency than non-teaching peers, consistently meeting or exceeding national standards. However, limitations such as smaller sample sizes, attrition rates, and PISA's narrow scope in terms of age and content coverage must be acknowledged.

Despite these constraints, meaningful trends can be seen. While the findings suggest a strong mathematical foundation among individuals who became teachers, effectiveness in teaching extends beyond proficiency alone. Future research could include assessments of pedagogical skills and statistical tests to more confidently determine significant differences between teachers and non-teachers. Thus, while the findings offer insights into teachers' mathematical strengths, they remain preliminary and should be paired with evaluations of teaching and communication skills to gauge true effectiveness in teaching mathematics.


# References

Baumert, J., Kunter, M., Blum, W., Brunner, M., Voss, T., Jordan, A., Klusmann, U., Krauss, S., Neubrand, M., & Tsai, Y.-M. (2010). Teachers’ Mathematical Knowledge, Cognitive Activation in the Classroom, and Student Progress. American Educational Research Journal, 47(1), 133-180. https://doi.org/10.3102/0002831209345157

Caro DH, Biecek P (2017). “intsvy: An R Package for Analyzing International Large-Scale Assessment Data.” Journal of Statistical Software, 81(7), 1-44. doi:10.18637/jss.v081.i07

Cowgill M (2024). ggdirectlabel: Make it Easier to Directly Label ggplot2 Charts Rather Than Using Legends. R package version 0.1.0.901

Cowgill M, Mackey W (2024). grattantheme: Create Graphs in the Grattan Institute Style. R package version 1.2.0

Gotsis, T. (2020). NSW school education: PISA 2018, socioeconomic background and proposals for reform. Parliament of New South Wales. https://www.parliament.nsw.gov.au/researchpapers/Pages/NSW-school-education-PISA-2018.aspx

Grolemund G, Wickham H (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), 1-25. URL: https://www.jstatsoft.org/v40/i03/

Hester J, Bryan J (2024). glue: Interpreted String Literals. R package version 1.7.0, https://CRAN.R-project.org/package=glue

Hill, H. C., Rowan, B., & Ball, D. L. (2005). Effects of Teachers’ Mathematical Knowledge for Teaching on Student Achievement. American Educational Research Journal, 42(2), 371-406. https://doi.org/10.3102/00028312042002371

Ilizaliturri R, Avvisati F, Keslair F (2023). Rrepest: An Analyzer of International Large Scale Assessments in Education. R package version 1.3.0, https://CRAN.R-project.org/package=Rrepest

Kay M (2024). “ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics.” IEEE Transactions on Visualization and Computer Graphics, 30(1), 414-424. doi:10.1109/TVCG.2023.3327195

Lumley T (2004). “Analysis of Complex Survey Samples.” Journal of Statistical Software, 9(1), 1-19

Lumley T (2010). Complex Surveys: A Guide to Analysis Using R. John Wiley and Sons

Lumley T (2024). survey: analysis of complex survey samples. R package version 4.4

Morsy, L., Khavenson, T., & Carnoy, M. (2018). How international tests fail to inform policy: The unsolved mystery of Australia’s steady decline in PISA scores. International Journal of Educational Development, 60, 60-79. https://doi.org/10.1016/j.ijedudev.2017.10.018

Norton, S. (2019). The relationship between mathematical content knowledge and mathematical pedagogical content knowledge of prospective primary teachers. Journal of Mathematics Teacher Education, 22(5), 489-514. https://doi.org/10.1007/s10857-018-9401-y

OECD. (2009). "Plausible Values", in OCED (Ed.), PISA Data Analysis Manual: SPSS (2nd ed., pp. 93-101). OECD Publishing. https://doi.org/10.1787/9789264056275-7-en

OECD. (2019). What can students do in mathematics? In OCED (Ed.), PISA 2018 Results (pp. 103-110). OECD Publishing. https://doi.org/10.1787/f649d3c2-en

OECD. (2023). What can students do in mathematics, reading and science? In OCED (Ed.), PISA 2022 Results (pp. 88-108). OECD Publishing. https://doi.org/10.1787/02f44b44-en

Okubo, T. (2022). "Theoretical considerations on scaling methodology in PISA." OECD Education Working Papers (Working Paper No. 282). OECD Publishing. https://doi.org/10.1787/c224dbeb-en

O'Hara-Wild M, Kay M, Hayes A (2024). distributional: Vectorised Probability Distributions. R package version 0.4.0, https://CRAN.R-project.org/package=distributional

Sievert C (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC Florida

Slowikowski K (2023). ggrepel: Automatically Position Non-Overlapping Text Labels with ggplot2. R package version 0.9.3, https://CRAN.R-project.org/package=ggrepel

Wickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York

Wickham H, Averick M, Bryan J, et al. (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686

Wickham H, Bryan J (2023). readxl: Read Excel Files. R package version 1.4.3, https://CRAN.R-project.org/package=readxl

Wickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.3, https://CRAN.R-project.org/package=dplyr

Wickham H, Henry L (2023). purrr: Functional Programming Tools. R package version 1.0.2, https://CRAN.R-project.org/package=purrr

Wickham H, Miller E, Smith D (2023). haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files. R package version 2.5.4, https://CRAN.R-project.org/package=haven

Wickham H, Pedersen T, Seidel D (2023). scales: Scale Functions for Visualization. R package version 1.3.0, https://CRAN.R-project.org/package=scales

Wilke C, Wiernik B (2022). ggtext: Improved Text Rendering Support for 'ggplot2'. R package version 0.1.2, https://CRAN.R-project.org/package=ggtext

Wu, M. (2022, June 7). Chapter 14 Estimating Population Characteristics - Part II: Plausible Values. A Course on Test and Item Analyses. Retrieved October 28, 2024, from https://www.edmeasurementsurveys.com/Ch14.html

Zhu H (2021). kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version 1.3.4, https://CRAN.R-project.org/package=kableExtra