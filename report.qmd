---
title: "Teaching Math Effectively: Insights from LSAY and PISA on Primary Teacher Proficiency"
author: Tashya Sathyajit (29672732)
format: pdf
number-sections: true
execute: 
  echo: false
  warning: false
  message: false
  wordcount: true
toc: true
---

```{r}

library(dplyr)
library(kableExtra)

```

# Abstract

This report investigates the mathematics performance of individuals who became primary school teachers in Australia, using the Longitudinal Surveys of Australian Youth (LSAY) and the Program For International Student Assessment (PISA) datasets. Results show that teachers consistently have a higher average math proficiency than non-teachers over time, with better performance across PISA testing years. However, they tend to score lower than professionals in fields like medicine, law, and engineering. Limitations include disproportionate teacher counts in some PISA years, potentional attrition bias, confounding variables such as socioeconomic status, and the evolving nature of math proficiency over time.

# Background and motivation

Over the past two decades, Australian students' performance in the mathematics components of international assessments like PISA have shown [significant declines](https://d1wqtxts1xzle7.cloudfront.net/57141870/Morsy__Khavenson__Carnoy__2018._How_international_tests_fail_to_inform_policy-libre.pdf?1533581345=&response-content-disposition=inline%3B+filename%3DHow_international_tests_fail_to_inform_p.pdf&Expires=1729912030&Signature=ApSPqfXhzY0QgniYXnR-sD6x-YnbhxaJd20T1EGQdRChDRaVLH-gqFf3iWzu9H~0Yv4ad2ls6nQvwxvAa5YzlyrwLK4YPd8N8jlhB0~nFbF~5CEqKBhG5RCthm5b2-ynja8eJSokxsfEEVjy-HiX3RCgxfZpE7UhYVAdOuqehKV4s3gNrEtGTd82J4RGt8VapK0MT6B~D066LmR~nR44sccPY7sX6UdqPg4sXkBtQKASI2qF4uvWLM5qkgvUHmsTBQvfEYNCc1NXe-3AzfLDwrsbVzszbb7J4-N8SEhDEXF3JRyJWXPubgeh8csxnYa73FFNbrkCcQnNiU4JIYgrXQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) as seen in @fig-pisadecline. These declines persist even after adjusting for changes in family academic resources, affecting students across all states, social groups, and school types (albeit with variations).This trend has raised concerns among educators, researchers and policymakers as mathematics proficiency is not only foundational for broader academic success but crucial for Australia's future economic competitiveness.

Research indicates a positive correlation between teacher mathematics knowledge (as assessed through certification exams or subject-matter tests) and [student achievement](https://dash.harvard.edu/bitstream/handle/1/37364487/Hill_Rowan_Ball_050405.pdf?sequence=2&isAllowed=y). This underscores the importance of qualified primary school teachers in improving children's math outcomes, supporting the notion that teachers with stronger mathematical knowledge are [likely to teach it more effectively.](https://www.researchgate.net/profile/Stephen-Norton-5/publication/322991092_The_relationship_between_mathematical_content_knowledge_and_mathematical_pedagogical_content_knowledge_of_prospective_primary_teachers/links/5a7bb457aca27233575b1e9d/The-relationship-between-mathematical-content-knowledge-and-mathematical-pedagogical-content-knowledge-of-prospective-primary-teachers.pdf)

While research consistently suggests that a teacher's proficiency in mathematics is a crucial determinant of their effectiveness in the classroom, impacting student achievement directly (Tatto et al. 2008, p. 19) the relationship between a teacher's subject knowledge and student performance is complex, and extends beyond the teacher's own content mastery. It involves the ability to translate that knowledge into effective pedagogical practices, adapt to curriculum needs, engage in productive professional conversations and address student misconceptions. When students are taught by teachers who have little mathematical content *and* pedagogical content knowledge, learning suffers (Baumert et al., 2010; Hill et al., 2005).

Despite these nuances, the positive correlation between teachers' competence in mathematics and student achievement is clear (Hill, H. C., Rowan, B., & Ball, D. L. (2005)). While this correlation does not strictly imply causation, understanding the mathematics proficiency of primary school teachers remains a critical step in improving student outcomes.

This analysis seeks to deepen the understanding of the mathematics proficiency of individuals who eventually become primary school teachers in Australia by leveraging longitudinal data from PISA and LSAY. By examining trends over time and comparing the math performance of primary school teachers, the broader Australian population and other professionals, this study aims to provide insights into the readiness of teachers to deliver effective math instruction. Understanding these trend is crucial, as they provide insights into the mathematics performance of individuals who later became primary school teachers, and may assist in guiding professional development initiatives focused on targeting and strengthening the mathematics proficiency of primary school teachers, ultimately leading to improved student outcomes across Australia.


![Decline in PISA Scores Across the Years](images/pisadecline.png){#fig-pisadecline}



# Objectives and significance

The primary objectives of this analysis was to utilise longitudinal survey data to understand the mathematics proficiency of students who eventually became primary school teachers in Australia. Using the LSAY and PISA datasets, the analysis seeks to address three main questions:

1.  **Teacher performance over time:** How well did individuals who became primary school teachers perform in mathematics across the years?
2.  **Comparison to other Professionals:** How are they performing in comparison to other 'professionals'?
3.  **Comparison to the General Population:** How do teachers' math scores compare to the rest of the sample (non primary school teachers)?

This analysis is significant as it provides critical insights into the math skills of individuals who become primary school teachers, a factor that can directly impact student outcomes. By identifying trends and proficiency gaps, the analysis can allow for a deeper understanding of the performance of primary school teachers in Australia and inform potential

# Methodology

Data was sourced from multiple LSAY cohorts (2003, 2006, 2009 and 2015), each containing demographic data from student responses and their linked performance in PISA. Data was selected from these years because post 2003, LSAY participants were recruited from schools that also took part in PISA, allowing for the linkage of PISA scores to LSAY data.

## How did we identify primary school teachers?

For each cohort, variables relating to kind of work were selected for each wave of LSAY data aforementioned. These were located using the LSAY data dictionary sourced [here.](https://www.lsay.edu.au/data/lsay-data-dictionary/data-dictionary)

For variables associated with the kind of work, the ANZSCO code `2412` was used to identify which observations (identified by the student ID) were primary school teachers. This variables used to filter can be seen in @tbl-kow.

```{r}
#| label: tbl-kow
#| tbl-cap: "Variables for 'What kind of work do you do in this job?' Questions"
kow <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LDD025, LED025, LFD025, LGD025, LHD029, LID029, LJD029, LKD029",
    "ANZSCO, LBD011, LCD024, LDD024, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "ANZSCO73, LBD014, LCD029, LDD029, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "LSAYID, LBEM004, LCD022, LDD022, LED022, LFD025, LGD022, LHD022"
  )
)

kable(kow, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(kow), hline_after = TRUE)

```


For better understanding of the legitimacy of this method, we then filtered those student IDs that responded with that ANZSCO code for variables regarding area of study. This was done to see whether the individuals who did become teachers also did some form of teaching degree pathway to get into the profession. This used ASCED code `70103` in variables relating to questions focused at main area of study/training in this course at any given year seen in @table-aos


```{r}
#| label: tbl-aos
#| tbl-cap: "Variables for Main Area of Study Questions"
aos <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LBCZ087, LCCZ087, LDCZ087, LEC087, LFC087, LGC087, LHC087, LIC087, LJC088, LKC088",
    "LBC087, LCC087, LDC087, LEC087, LFC087, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LBC089, LCC088, LDC088, LEC088, LFC088, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LCC088, LDC088, LEC088, LFC088, LGC088, LHC088"
  )
)

kable(aos, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(aos), hline_after = TRUE)
```


## How did we complete the Mathematical Proficiency Analysis?

The mathematics performance of primary school teachers was assessed using PISA scores. The analysis aimed to compare teachers' scores against the national proficiency standard which has been set at a proficiency level 3 or [482 PISA points](https://www.parliament.nsw.gov.au/researchpapers/Documents/PISA%20paper.pdf). The analysis incorporated survey weights to ensure representativeness of the data and address differences in sample sizes across cohorts. It also had to account for the use of plausible values undertaken by PISA.


## Working with Sampling Weights and Plausible Values Accurately

To handle PISA's plausible values and sampling weights, the analysis utilised [`intsvy`](https://github.com/eldafani/intsvy) and [`rrepest`](https://github.com/cran/Rrepest) R packages, which are designed to accurately estimate statistics under the complex survey design used by PISA.

These allowed for the calculation of mean scores, confidence intervals and standard errors while accounting for the inherent variability within the dataset.

**But what are plausible values?**

International surveys such as PISA report student performance using plausible values (PVs). Plausible values are a statistical technique that represent uncertainty in measurements by generating a range of possible values a student may have obtained as opposed to a single estimate. This approach assists in conveying the variability in student performance more accurately, considering that not all students answer the same questions (PISA is a large test) or have the same background information. 

To derive PVs, PISA uses [Item Response Theory (IRT)](https://www.oecd-ilibrary.org/docserver/c224dbeb-en.pdf?expires=1730041650&id=id&accname=guest&checksum=6717B0D94EE373655D9733F88B98A43F), which predicts the likelihood of a student answering a question correctly by assessing the difficulty of each question and how well it distinguishes between students of different ability levels. For example, if the question is deemed a relatively conceptually 'easy' question, and the estimated ability of the student is high, the model predicts a high probability of a correct response. PISA can determine if a question is 'easy' or 'hard' based on the statistical analysis of response patterns from a broad group of students, using IRT to estimate how challenging each question is relative to the separate proficiency levels.

Looking at @fig-pvs retrieved from [Wu (2022)](https://www.edmeasurementsurveys.com/Ch14.html), the process starts with a prior distribution. This is an initial estimate of the student's proficiency based on background variables like socioeconomic status or parental education. This prior estimation serves as a rough starting point that as the students begin to answer questions, get fintuned as PISA updates this estimate using student responses and resulting in a posterior distribution. This updating distribution combines that initial prior estimate with the observed test results.

After this, multiple imputed values are taken from this posterior distribution. Essentially, PISA generates plausible values that are a representation of the range of abilities a student [may reasonably have.](https://www.oecd-ilibrary.org/docserver/9789264056275-7-en.pdf?expires=1730036478&id=id&accname=guest&checksum=662C072FDF248EAFF7659869C306F91C#:~:text=It%20has%20been%20noted%20that,a%20student's%20%CE%B8%20is%20estimated.)


![Prior and Posterior Distributions](images/pvs.png){#fig-pvs}


## How did the packages account for this?











# Results

As earlier mentioned in the report, the project sought to address three key questions through the exploration of the mathematical performance of indiviudals who became primary school teachers. 

### How well did individuals who became primary school teachers perform in mathematics across the years?

The analysis reveals that individuals who eventually became primary school teachers consistently performed better in mathematics compared to their non-teaching peers over the years. To quantify this, the weighted average proficiency of teachers and non-teachers was calculated taking into consideration the total number of teachers and non-teachers which served as the basis for weighting the average proficiency scores. 

The results show as depicted in @fig-weightedprof, and illustrates that over the years considered in this analysis, just over 80% of teachers met or exceed the national proficiency standard. In comparison, just over 60% of the non-teachers met or exceeded the national proficiency standard of 482 PISA points. This may suggest that individuals who eventually become primary school teachers have a higher proficiency in mathematics compared to their non-teaching peers.



![Weighted Average of Performance Across Years: Teachers vs Non-Teachers](images/weightedprof.png){#fig-weightedprof}




### How do teachers' math scores compare to the rest of the sample (non primary school teachers)?

When trying to see how teachers performed in each year, we were able to find that teachers tended to perform better across each year of testing. As seen in @fig-acrossyears, teachers (red) tend to do consistently better than non-teachers (orange) across the years. 

The boxplots shows that the median, interquartile range, and 10th and 90th percentiles. The boxplots for non-teachers have longer whiskers, indicating a broader range of scores and greater variability in math proficiency compared to teachers. The wider spread suggests that non-teachers include individuals with a diverse range of mathematical skills, spanning from very low to very high.

For teachers, the whiskers are shorter, indicating more consistency in their math scores. However, the standard error (SE) for the teacher group is higher, this is likely due to their smaller sample size, which increases the uncertaining in estimating the average score.

Ultimately, while non-teachers exhibit more variability in individual scores, the higher standard error for teachers reflects the effect of smaller sample sizes on the precision of the average, not necessarily a greater variability in the scores itself.


![Proficiency of Teachers vs Non-Teachers Across Years](images/profacrossyears.png){#fig-acrossyears}



### How are they performing in comparison to other 'professionals'?

The project had a smaller focus on understanding how teachers' mathematical proficiency compared to other professionals, driven largely by curiosity rather than formal research aims. The selection of professional categories was based on existing reports, and the analysis was conducted on the 2003 PISA cohort, the year with the largest sample of teachers. However this still had its limitations due to the smaller sample size of the relative professional groups within a total sample of 10,370, limiting the overall robustness of the findings.

Despite this, as seen in @fig-professionals, teachers performed lower than most other professional groups in terms of average mathematical proficiency at age 15. Professions such as medical practitioners, legal professionals, engineers and auditors scored higher on average, indicating better mathematical capability amongst these groups. However, teachers school higher than the 'Other' category, which represents the broader population to aligned with the specific professions of interest. This suggests that while teachers were not among the highest performing professional groups, they seemed to demonstrate better mathematical proficiency than the general cohort.

Despite these findings, it is important to consider the larger confidence intervals observed for certain professional groups, such as legal professionals, engineers, and auditors. These wider intervals indicate greater uncertainty in the estimates and may result from factors like smaller sample sizes and higher variability within these groups.


![Teachers in Comparison to Other Professionals](images/professionals.png){#fig-professionals}


# Discussion





# Limitations

![Count of Teachers per Year](images/teachercount.png){#fig-teachercount}


# Conclusion
