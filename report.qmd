---
title: "Teaching Math Effectively: Insights from LSAY and PISA on Primary Teacher Proficiency"
author: Tashya Sathyajit (29672732)
format: 
  pdf:
    number-sections: true
    toc: true
    include-in-header:
      text: |
        \usepackage{lipsum}
        \usepackage{setspace}
        \onehalfspacing
        \linespread{2}
    df-print: kable
    highlight-style: zenburn
fontsize: 12pt
geometry: margin=1in
execute: 
  echo: false
  warning: false
  message: false
  wordcount: true
---


```{r}
# necessary packages
library(dplyr)
library(kableExtra)
library(knitr)
```

# Abstract

This report investigates the mathematics performance of individuals who became primary school teachers in Australia, using the Longitudinal Surveys of Australian Youth (LSAY) and the Program For International Student Assessment (PISA) datasets. This analysis was done for the contribution to Grattan Institute's report on teacher proficiency in mathematics. Results show that teachers consistently have a higher average math proficiency than non-teachers over time, with better performance across PISA testing years. However, they tend to score lower than professionals, particularly in STEM fields. Limitations include disproportionate teacher counts in some PISA years, potential attrition bias, confounding variables such as socioeconomic status, and the evolving nature of math proficiency over time.

# Background and motivation

Over the past two decades, Australian students' performance in the mathematics components of international assessments like PISA have shown significant declines (Morsy et al., 2018) as seen in @fig-pisadecline. These declines persist even after adjusting for changes in family academic resources, affecting students across all states, social groups, and school types (albeit with variations). This trend has raised concerns among educators, researchers and policymakers as mathematics proficiency is not only foundational for broader academic success but crucial for Australia's future economic competitiveness.

Research indicates a positive correlation between teacher mathematics knowledge (as assessed through certification exams or subject-matter tests) and student achievement (Hill et al., 2005) . This underscores the importance of qualified primary school teachers in improving children's math outcomes, supporting the notion that teachers with stronger mathematical knowledge are likely to teach it more effectively (Norton, 2019).

While research consistently suggests that a teacher's proficiency in mathematics is a crucial determinant of their effectiveness in the classroom, impacting student achievement directly (Morsy et al., 2018) the relationship between a teacher's subject knowledge and student performance is complex, and extends beyond the teacher's own content mastery. It involves the ability to translate that knowledge into effective pedagogical practices, adapt to curriculum needs, engage in productive professional conversations and address student misconceptions. When students are taught by teachers who have little mathematical content *and* pedagogical content knowledge, learning suffers (Baumert et al., 2010; Hill et al., 2005).

Despite these nuances, the positive correlation between teachers' competence in mathematics and student achievement is clear (Hill et al., 2005). While this correlation does not strictly imply causation, understanding the mathematics proficiency of primary school teachers remains a critical step in improving student outcomes.

This analysis seeks to deepen the understanding of the mathematics proficiency of individuals who eventually become primary school teachers in Australia by leveraging longitudinal data from PISA and LSAY. By examining trends over time and comparing the math performance of primary school teachers, the broader Australian population and other professionals, this study aims to provide insights into the readiness of teachers to deliver effective math instruction. Understanding these trends is crucial, as they provide insights into the mathematics performance of individuals who later became primary school teachers, and may assist in guiding professional development initiatives focused on targeting and strengthening the mathematics proficiency of primary school teachers, ultimately leading to improved student outcomes across Australia.


![Decline in PISA Scores Across the Years](images/pisadecline.png){#fig-pisadecline}



# Objectives and significance

The primary objective of this analysis was to utilise longitudinal survey data to understand the mathematics proficiency of students who eventually became primary school teachers in Australia. Using the LSAY and PISA datasets, the analysis seeks to address three main questions:

1.  **Teacher performance over time:** How well did individuals who became primary school teachers perform in mathematics across the years?
2.  **Comparison to other Professionals:** How are they performing in comparison to other 'professionals'?
3.  **Comparison to the General Population:** How do teachers' math scores compare to the rest of the sample (non-primary school teachers)?

This analysis is significant as it provides critical insights into the math skills of individuals who become primary school teachers, a factor that can directly impact student outcomes. By identifying trends and proficiency gaps, the analysis can allow for a deeper understanding of the performance of primary school teachers in Australia and inform potential policy recommendations. 

# Methodology

Data was sourced from multiple LSAY cohorts: 2003, 2006, 2009 and 2015. Each dataset contains demographic data from student responses and their linked performance in PISA. These years were selected because, from 2003 onwards, LSAY participants were recruited from schools that also took part in PISA, allowing for the linkage of PISA scores to LSAY data.

## How did we identify primary school teachers?

To identify primary school teachers for each cohort, we examined variables related to occupation across each LSAY cohort. The [data dictionary](https://www.lsay.edu.au/data/lsay-data-dictionary/data-dictionary) was used to locate the relevant variables, as found in @tbl-kow. These variables correspond to responses about the type of work the respondents did.

Individuals were flagged as primary school teachers if they reported an ANZSCO code of `2412`, which represents primary school teachers. This flag distinguished those who eventually became primary school teachers from those who did not. Respondents with other occupation codes were classified as non-teachers.

```{r}
#| label: tbl-kow
#| tbl-cap: "Variables for 'What kind of work do you do in this job?' Questions"
kow <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LDD025, LED025, LFD025, LGD025, LHD029, LID029, LJD029, LKD029",
    "ANZSCO, LBD011, LCD024, LDD024, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "ANZSCO73, LBD014, LCD029, LDD029, LED029, LFD029, LGD029, LHD029, LID029, LJD029, LKD029",
    "LSAYID, LBEM004, LCD022, LDD022, LED022, LFD025, LGD022, LHD022"
  )
)

kable(kow, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(kow), hline_after = TRUE)

```

### What did these teachers study? 

To understand the educational backgrounds of individuals who later became primary school teachers, fields of study were analysed using variables detailed in @tbl-aos. The analysis focused on identifying whether respondents pursued primary school education, coded as `7301` in accordance with the [Australian Standard Classification of Education (ASCED)](https://www.abs.gov.au/statistics/classifications/australian-standard-classification-education-asced/latest-release).

Additionally, the analysis explored whether respondents who became teachers had previously studied fields outside of education before transitioning to teaching. This was done with a similar approach of examining ASCED scores reported across waves of those who were identified as teachers. @tbl-aosunique provides a summary of the fields of study identified and highlights cases where individuals shifted from other fields to primary education.

```{r}
#| label: tbl-aos
#| tbl-cap: "Variables for Main Area of Study Questions"
aos <- data.frame(
  Year = c(2003, 2006, 2009, 2015),
  Variables = c(
    "LBCZ087, LCCZ087, LDCZ087, LEC087, LFC087, LGC087, LHC087, LIC087, LJC088, LKC088",
    "LBC087, LCC087, LDC087, LEC087, LFC087, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LBC089, LCC088, LDC088, LEC088, LFC088, LGC088, LHC088, LIC088, LJC088, LKC088",
    "LCC088, LDC088, LEC088, LFC088, LGC088, LHC088"
  )
)

kable(aos, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(aos), hline_after = TRUE)
```


```{r}
#| label: tbl-aosunique
aos_unique <- data.frame(
  Code = c(
    70101, 70103, 70109, 70199, 70115, 90503, 91703, 92101, 100101, 80101,
    100705, 100103, 80399, 70105, 91701, 60901, 61399, 60101, 81101, 91523, 90513,
    90701, 79999, 60501, 61799
  ),
  Field_of_Education = c(
    "Teacher Education: Early Childhood",
    "Teacher Education: Primary",
    "Teacher Education: Vocational Education and Training",
    "Teacher Education, n.e.c.",
    "English as a Second Language Teaching",
    "Children's Services",
    "Religious Studies",
    "Sport and Recreation Activities",
    "Music",
    "Accounting",
    "Written Communication",
    "Drama and Theatre Studies",
    "Business and Management, n.e.c.",
    "Teacher Education: Secondary",
    "Philosophy",
    "Optometry",
    "Public Health, n.e.c.",
    "General Medicine",
    "Banking and Finance",
    "Literature",
    "Counselling",
    "Psychology",
    "Education, n.e.c.",
    "Pharmacy",
    "Rehabilitation Therapies, n.e.c."
  )
)

kable(aos_unique, 
      booktabs = TRUE, 
      align = "l") %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  column_spec(2, width = "30em") %>%
  row_spec(0, bold = TRUE, color = "white", background = "gray") %>%
  row_spec(1:nrow(aos_unique), hline_after = TRUE)
```


## How did we explore teacher performance relative to other professionals?

This part of the analysis was driven more by curiosity than formal research aims, exploring how primary school teachers' mathematics proficiency compares to that of other professionals. Due to this, the analysis focused only on the 2003 PISA cohort which had the largest sample of teachers (view @fig-teachercount) and of that cohort, the final wave. By examining the last wave of this cohort, the analysis captures participants' final professional roles, though this approach results in a lower teacher count than the total count for the entire 2003 cohort as seen in @tbl-professionals.

To define professional categories, the analysis used the ANZSCO codes sourced from the Australian Bureau of Statistics [website](https://www.abs.gov.au/statistics/classifications/anzsco-australian-and-new-zealand-standard-classification-occupations/latest-release). The respondent's occupation code response to variable `LKD029` corresponding to LSAY question relating to kind of work was used to identify whether they belonged to one of the professional groups of interest. A new variable was established which created a flag for individuals based on their occupation code (e.g. Teachers coded as '1', while other professional groups were coded '2' to '6' in accordance to their ANZSCO code). 


```{r}
#| label: tbl-professionals

professionals_table <- data.frame(
  Professionals = c("Primary School Teacher", 
                    "Medical Practitioners", 
                    "Midwifery and Nursing Professionals", 
                    "Legal Professionals", 
                    "Accounts Auditors and Company Secretaries", 
                    "Engineering Professionals"),
  ANZSCO_Code = c(2412, 253, 254, 271, 221, 233),  
  Count_n = c(82, 48, 94, 50, 92, 88)  
)

professionals_table <- professionals_table %>%
  rename(
    `ANZSCO Code` = ANZSCO_Code,
    Count = Count_n
  )


professionals_table %>%
  kable("latex", booktabs = TRUE, caption = "Professionals, Corresponding ANZSCO Codes, and Counts") %>%
  kable_styling(latex_options = c("striped"))

```


## How did we complete the Mathematical Proficiency Analysis?

The mathematics performance of primary school teachers was assessed using PISA scores. The analysis aimed to compare teachers' scores against the national proficiency standard which has been set at a [proficiency level 3](https://www.oecd-ilibrary.org/sites/5f07c754-en/1/2/7/index.html?itemId=/content/publication/5f07c754-en&_csp_=6aa84fb981b29e81b35b3f982f80670e&itemIGO=oecd&itemContentType=book) or [482 PISA points](https://www.parliament.nsw.gov.au/researchpapers/Documents/PISA%20paper.pdf). The analysis incorporated survey weights to ensure representativeness of the data and address differences in sample sizes across cohorts. It also had to account for the use of plausible values undertaken by PISA.

::: {.callout-note}
## **At Level 3 Proficiency in Mathematics**
"At Level 3, students can execute clearly described procedures, including those that require sequential decisions. Their interpretations are sufficiently sound to be a base for building a simple model or for selecting and applying simple problem-solving strategies. Students at this level can interpret and use representations based on different information sources and reason directly from them. They typically show some ability to handle percentages, fractions and decimal numbers, and to work with proportional relationships. Their solutions reflect that they have engaged in basic interpretation and reasoning."
:::

## Working with Sampling Weights and Plausible Values Accurately


International surveys such as PISA report student performance using plausible values (PVs). Plausible values are a statistical technique that represents uncertainty in measurements by generating a range of possible values a student may have obtained as opposed to a single estimate. This approach assists in conveying the variability in student performance more accurately, considering that not all students answer the same questions (PISA is a large test) or have the same background information. 

To derive PVs, PISA uses [Item Response Theory (IRT)](https://www.oecd-ilibrary.org/docserver/c224dbeb-en.pdf?expires=1730041650&id=id&accname=guest&checksum=6717B0D94EE373655D9733F88B98A43F), which predicts the likelihood of a student answering a question correctly by assessing the difficulty of each question and how well it distinguishes between students of different ability levels. For example, if the question is deemed a relatively conceptually 'easy' question, and the estimated ability of the student is high, the model predicts a high probability of a correct response. PISA can determine if a question is 'easy' or 'hard' based on the statistical analysis of response patterns from a broad group of students, using IRT to estimate how challenging each question is relative to the separate proficiency levels, as can be seen [here](https://www.oecd-ilibrary.org/docserver/02f44b44-en.pdf?expires=1730101739&id=id&accname=guest&checksum=903DBFF4325A31EBBCC87BE873992B35) (OECD, 2023).

Looking at @fig-pvs retrieved from [Wu (2022)](https://www.edmeasurementsurveys.com/Ch14.html), the process starts with a prior distribution. This is an initial estimate of the student's proficiency based on background variables like socioeconomic status or parental education. This prior estimation serves as a rough starting point that as the students begin to answer questions, get finetuned as PISA updates this estimate using student responses and resulting in a posterior distribution. This updating distribution combines that initial prior estimate with the observed test results.

After this, multiple imputed values are taken from this posterior distribution. These plausible values are thus a representation of the range of abilities a student may reasonably have (OECD, 2009) and are normally five to 10 plausible values per student for each domain.


![Prior and Posterior Distributions](images/pvs.png){#fig-pvs}

The [`intsvy`](https://github.com/eldafani/intsvy) and [`rrepest`](https://github.com/cran/Rrepest) R packages were employed to handle the complex survey design of PISA data, ensuring accurate estimation of statistics such as mean scores, confidence intervals, and standard errors. These packages allowed for the calculation of weighted averages, taking into account the total number of teachers and non-teachers in each cohort to provide a more representative estimate of the average proficiency scores.

As PISA uses sampling weights to account for its stratified, multi-staged sampling design which includes adjustments for things such as non-response, school selection and student participation, the `Rrepest` package applies these weights when calculating summary statistics. It will iteratively apply each set of replicate weights to generate multiple estimates. These estimates are then combined to produced accurate standard errors that reflect the variability introduced by PISA's sampling structure. This approach also ensures that results are not biased by unequal probabilities of selection and attempt to provide more representativeness.

In the case of this analysis, the `Rrepest` package was utilised mostly in determining our summary statistics, although results were crossed checked with the `intsvy` package. The package included functions that cycled through each plausible value, averaging the results while simultaneously applying the survey weights to maintain representatives. This combined approach of incorporating plausible values and weights ensures that the estimates are statistically valid, accounting for uncertainty in measurement and complex survey design. It was also [recommended](https://www.oecd.org/en/about/programmes/pisa/how-to-prepare-and-analyse-the-pisa-database.html) by the OECD for handling PISA data and plausible values.


# Results

As earlier mentioned in the report, the project aimed to explore three key questions regarding the mathematical performance of individuals who eventually became primary school teachers.

### How well did individuals who became primary school teachers perform in mathematics across the years?

The analysis reveals that individuals who eventually became primary school teachers consistently performed better in mathematics compared to their non-teaching peers over the years. To quantify this, the weighted average proficiency of teachers and non-teachers was calculated, considering the total number of each group to ensure more accurate average scores.

The results, as shown in @fig-weightedprof, indicate that just over 80% of teachers met or exceeded the national proficiency standard of 482 PISA points, compared to just over 60% of non-teachers. This substantial difference suggests that individuals who eventually become primary school teachers possess stronger mathematical skills than their non-teaching peers from the same cohort. These results highlight the higher mathematical readiness of future teachers, which may have implications for their effectiveness in the classroom.

However, an important limitation to consider is that the smaller sample of teachers may reduce the observed variability, potentially skewing results if high-performing individuals are overrepresented. @fig-probdist shows that in the 2003 sample, there is less variability within the teacher group compared to the non-teacher group. This potential selection bias could lead to an inflated perception of average teacher math proficiency, as the sample might not fully capture the broader range of abilities likely found in the general teacher population.


```{r}
#| label: tbl-profscores
pisa_results <- data.frame(
  Year = c(2003, 2003, 2006, 2006, 2009, 2009, 2015, 2015),
  Teacher = c("False", "True", "False", "True", "False", "True", "False", "True"),
  Mean = c(530.3, 554.0, 518.6, 550.7, 513.7, 552.7, 494.0, 519.4),
  SE_Mean = c(2.4, 10.5, 3.0, 10.4, 2.7, 7.2, 1.8, 20.2),
  Std = c(92.9, 64.8, 87.9, 67.2, 95.1, 63.9, 92.5, 61.0),
  SE_Std = c(2.4, 5.3, 1.2, 8.2, 2.5, 8.4, 0.9, 19.0)
)
pisa_results %>%
  kable("latex", booktabs = TRUE, caption = "PISA Teacher vs Non-Teacher Math Proficiency") %>%
  kable_styling(latex_options = c("striped"))

```



![Weighted Average of Performance Across Years: Teachers vs Non-Teachers](images/weightedprof.png){#fig-weightedprof}

![Probability Distribution of 2003 Mathematics PISA Scores](images/probdist.png){#fig-probdist}


### How do teachers' math scores compare to the rest of the sample (non primary school teachers)?

@fig-acrossyears shows that teachers tend to perform better than non-teachers across each year of testing.

This is demonstrated by a consistently higher median score for teachers (red) compared to non-teachers (orange) over the years. The boxplots reveal differences in score distribution, with non-teachers showing longer whiskers, indicating a broader range of math scores and greater variability. This wider spread suggests that non-teachers have a more diverse range of mathematical abilities.

In contrast, the boxplots for teachers display slightly shorter whiskers, indicating more consistent scores among this group. However, as seen in @tbl-profscores, the standard error (SE) for teachers is higher, likely due to the smaller sample size. This increases the uncertainty in estimating their average score but does not imply greater variability within the teacher group.

![Proficiency of Teachers vs Non-Teachers Across Years](images/profacrossyears.png){#fig-acrossyears}

### How are they performing in comparison to other 'professionals'?

The project had a smaller focus on understanding how teachers' mathematical proficiency compared to other professionals, driven largely by curiosity rather than formal research aims. The selection of professional categories was based on existing reports, and the analysis was conducted on the 2003 PISA cohort, the year with the largest sample of teachers. However this still had its limitations due to the smaller sample size of the relative professional groups within a total sample of 10,370 as seen in @tbl-professionals.

@fig-professionals represents the average PISA scores for teachers and other professionals, along with 90 percent confidence intervals. The analysis suggested that teachers performed lower than most other professional groups in terms of average mathematical proficiency at age 15. Professions such as medical practitioners, legal professionals, engineers and auditors scored higher on average, indicating better mathematical capability among these groups. However, teachers scored higher than the 'Other' category, which represents the broader population not within aforementioned professions. This suggests that while teachers were not among the highest performing professional groups, they seemed to demonstrate better mathematical proficiency than the general cohort.

Despite these findings, it is important to consider the larger confidence intervals observed for some professional groups, such as legal professionals, engineers and auditors. These wider intervals primarily reflect greater uncertainty in the estimates due to smaller sample sizes and potential sample bias within these groups. This increased uncertainty makes precise comparisons more challenging and difficult to ascertain with confidence the true average performance of these professional groups.

![Teachers in Comparison to Other Professionals](images/professionals.png){#fig-professionals}


# Discussion, Limitations and Future Research


The findings of this analysis provide insights into the mathematical proficiency of individuals who eventually become primary school teachers, indicating a generally higher proficiency than their non-teaching peers across multiple years. Teachers tend to meet or exceed  national proficiency standards more frequently than non-teachers, suggesting a relatively stronger foundation in mathematical skills. This trend is evident over time, as teachers consistently achieve higher median scores with less variability compared to non-teachers, as seen in @fig-acrossyears.

The more consistent performance among teachers is suggested by the slightly shorter whiskers in the boxplots, indicating a relatively uniform level of mathematical competence. In contrast, the longer whiskers for non-teachers reflect greater variability, indicating a broader range of skills. However, it is important to note that the whiskers for both groups are relatively long, suggesting a wide distribution of scores overall, with non-teachers demonstrating only slightly more spread. The higher standard error for teachers, meanwhile, is primarily due to their significantly smaller sample size, increasing uncertainty in the estimated average scores. This however, reflects the inherent limitations of sample size than actual variability in proficiency.

When comparing teachers to other professional groups, as seen in @fig-professionals, the analysis reveals that while future teachers performed better than the general population, they performed lower than medical practitioners, legal professionals, engineers, and auditors and performed quite similarly to midwifery and nursing professionals. This aligned with a preconceived idea that STEM professions would attract students with more confidence or proficiency in mathematics. However, the large confidence intervals observed for some professional groups highlight greater uncertainty of where the true average lies due to smaller sample sizes and potential variability within these groups.

However, there are some considerations that are important to pay mind to upon the results of this analysis. Despite the relatively strong performance of teachers in mathematics, as aforementioned, effective teaching requires more than just content knowledge. Research frameworks emphasise that teachers must understand how to present content effectively, integrate it with the curriculum, and adapt it to diverse learning styles (Norton, 2018). Additionally, although we can see that individuals who became teachers in later life had a higher proficiency in mathematics when compared to the general population (save for the professions explored deeper), there are limitations to the analysis.


![Count of Teachers per Year](images/teachercount.png){#fig-teachercount}

**Small sample size for teachers and professional groups:**

The sample size for teachers as seen in @fig-teachercount and other professionals as seen in @tbl-professionals was relatively small, which limits the robustness of the findings. The smaller sample size is also associated with larger standard errors, increasing uncertainty in the estimated averages of those groups. The lack of statistical power due to small sample sizes may have affected the precision of the estimates, potentially leading to biased results or challenges in generalising the results.

**Attrition Rate in Longitudinal Data:**

The analysis is based on longitudinal data, which is inherently prone to attrition over time. As participants drop out of the study, the sample size decreases which may introduce potential bias and affect the reliability of the results. Further, the later PISA cohorts haven't completed all the waves which contributes to the smaller sample size for teachers in the later years. This attrition bias may have influenced the results, particularly in the later years, and may have implications for the generalisability of the findings.

**Scope of PISA:**

As mentioned prior, while PISA tests part of the expected learning outcomes of for mathematics, it does not test how good an individual will be at conveying this knowledge to others or the entire range of mathematical knowledge required in teacher training or professional practice. This could suggest that although individuals who performed well in PISA who later became teachers may be good at mathematics, they may not necessarily be good at teaching it.

Additionally, PISA only tracks individuals from the age of 15 to 25, and this does not account for the many other ways individuals go into the pathway of being teachers. For example, mathematical proficiency can be enhanced through alternative pathways or lived work experience before the individual moved into teaching. This could also suggest that the analysis may not capture the full range of individuals who become teachers and their mathematical proficiency.

**High Standard Error:**

The high standard errors observed in the estimates for teachers ans some pro


For the project, to determine whether differences in mathematical proficiency between teachers and non-teachers statistically significant, a series of Welch's t-tests were conducted for each year. A Welch's t-test was deemed sufficient for this analysis, as it accommodates differences variances and sample sizes between groups. The t-tests generally showed statistically significant differences in average scores, with teachers consistently outperforming non-teachers in the years analysed. However, this should be treated as preliminary statistical analysis due to limitations prior mentioned and methodological considerations behind the reasoning to not include the results of these t-tests include:

-  **Small sample size:** There are relatively very small sample sizes for teachers, especially in latter years due to not all waves being released for the cohort (e.g. there are only 27 teachers in the 2015 sample). 

-  **Normality assumption:** The t-test assumes a normal distribution of scores within each group, but this assumption may not hold, especially given the small teacher samples. This could affect the validity of the test results. As seen in @fig-probdist, which represents *only* the 2003 sample, the distribution of teacher scores (orange) shows slight skewness and a narrower spread, indicating potential deviations from normality. The reduced sample size may also underrepresent true variability among teacher scores, further complicating interpretation.

Next steps could build upon this with the potential use of non-parametric tests or bootstrapping techniques.


This analysis reveals that individuals who later became primary school teachers generally exhibit higher mathematical proficiency than non-teaching peers, consistently meeting or exceeding national standards. However, limitations such as smaller sample sizes, attrition rates, and PISA's narrow scope in terms of age and content coverage must be acknowledged.Despite these constraints, meaningful trends can be seen. In particular, teachers achieve stronger median scores and less variability. Yet, they still fall behind STEM professionals. 

Although these findings suggest a strong basis for the individuals in this sample that eventually became teachers, teacher effectiveness extends beyond proficiency, emphasising the need for future research to include pedagogical skills and statistical tests that can determine with more confidence whether the difference between teachers and non-teachers are statistically significant. Thus, while the findings offer insights into teachers' mathematical strengths, they remain preliminary and should be used in conjunction with assessments that indicate how well these individuals would be able to teach and communicate mathematical concepts effectively.


# References

Baumert, J., Kunter, M., Blum, W., Brunner, M., Voss, T., Jordan, A., Klusmann, U., Krauss, S., Neubrand, M., & Tsai, Y.-M. (2010). Teachers’ Mathematical Knowledge, Cognitive Activation in the Classroom, and Student Progress. American Educational Research Journal, 47(1), 133-180. https://doi.org/10.3102/0002831209345157 


Gotsis, T. (2020). NSW school education: PISA 2018, socioeconomic background and proposals for reform. https://www.parliament.nsw.gov.au/researchpapers/Pages/NSW-school-education-PISA-2018.aspx


Hill, H. C., Rowan, B., & Ball, D. L. (2005). Effects of Teachers’ Mathematical Knowledge for Teaching on Student Achievement. American Educational Research Journal, 42(2), 371-406. https://doi.org/10.3102/00028312042002371


Morsy, L., Khavenson, T., & Carnoy, M. (2018). How international tests fail to inform policy: The unsolved mystery of Australia’s steady decline in PISA scores. International Journal of Educational Development, 60, 60-79. https://doi.org/https://doi.org/10.1016/j.ijedudev.2017.10.018


Norton, S. (2019). The relationship between mathematical content knowledge and mathematical pedagogical content knowledge of prospective primary teachers. Journal of Mathematics Teacher Education, 22(5), 489-514. https://doi.org/10.1007/s10857-018-9401-y


OECD. (2009). Plausible Values. https://doi.org/doi:https://doi.org/10.1787/9789264056275-7-en


Okubo, T. (2022). Theoretical considerations on scaling methodology in PISA.https://doi.org/doi:https://doi.org/10.1787/c224dbeb-en



 Wu, M. (2022, June 7) Chapter 14 Estimating Population Characteristics - Part II: Plausible Values. A Course on Test and Item Analyses. Retrieved October 28, 2024, from https://www.edmeasurementsurveys.com/Ch14.html

